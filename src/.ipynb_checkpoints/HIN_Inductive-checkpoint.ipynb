{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ffa63d-f28d-4c9e-b267-9b8fa405a287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aemad/.envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[17716, 1639], edge_index=[2, 105734], y=[17716])\n"
     ]
    }
   ],
   "source": [
    "from HIN_Inductive import * \n",
    "from TeacherHIN import Teacher\n",
    "#from inductive_hin import Student\n",
    "from typing import Optional\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8c5dc2-f588-4156-b4f6-80e07c7cc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomNodeSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d723b6b-aca1-4878-a383-fb7749933485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(data, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "def open_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a119ec-dada-46ff-8ee7-04f10d180ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = \"/data/pakdd2023/\"\n",
    "# name = \"DBLP\"\n",
    "# dataset = IMDB(root+name, name)\n",
    "# data = dataset.data\n",
    "# print(data)\n",
    "# data['conference']['x'] = torch.rand(data['conference'].num_nodes, 128)\n",
    "# #print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17a9ed-397d-4349-ae24-7919838ff47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad33dd8-ccdb-4182-ac5e-1b34ba2bc754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mpaper\u001b[0m={\n",
      "    x=[3025, 1902],\n",
      "    y=[3025],\n",
      "    train_mask=[3025],\n",
      "    test_mask=[3025]\n",
      "  },\n",
      "  \u001b[1mauthor\u001b[0m={ x=[5959, 1902] },\n",
      "  \u001b[1msubject\u001b[0m={ x=[56, 1902] },\n",
      "  \u001b[1mterm\u001b[0m={ num_nodes=1902 },\n",
      "  \u001b[1m(paper, cite, paper)\u001b[0m={ edge_index=[2, 5343] },\n",
      "  \u001b[1m(paper, ref, paper)\u001b[0m={ edge_index=[2, 5343] },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 9949] },\n",
      "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 9949] },\n",
      "  \u001b[1m(paper, to, subject)\u001b[0m={ edge_index=[2, 3025] },\n",
      "  \u001b[1m(subject, to, paper)\u001b[0m={ edge_index=[2, 3025] },\n",
      "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 255619] },\n",
      "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 255619] }\n",
      ")\n",
      "HeteroData(\n",
      "  \u001b[1mpaper\u001b[0m={\n",
      "    x=[3025, 1902],\n",
      "    y=[3025],\n",
      "    train_mask=[3025],\n",
      "    test_mask=[3025]\n",
      "  },\n",
      "  \u001b[1mauthor\u001b[0m={ x=[5959, 1902] },\n",
      "  \u001b[1msubject\u001b[0m={ x=[56, 1902] },\n",
      "  \u001b[1mterm\u001b[0m={\n",
      "    num_nodes=1902,\n",
      "    x=[1902, 1]\n",
      "  },\n",
      "  \u001b[1m(paper, cite, paper)\u001b[0m={ edge_index=[2, 5343] },\n",
      "  \u001b[1m(paper, ref, paper)\u001b[0m={ edge_index=[2, 5343] },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 9949] },\n",
      "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 9949] },\n",
      "  \u001b[1m(paper, to, subject)\u001b[0m={ edge_index=[2, 3025] },\n",
      "  \u001b[1m(subject, to, paper)\u001b[0m={ edge_index=[2, 3025] },\n",
      "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 255619] },\n",
      "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 255619] }\n",
      ")\n",
      "10942\n",
      "547872\n"
     ]
    }
   ],
   "source": [
    "root = \"./data/pakdd2023/\"\n",
    "name = \"ACM\"\n",
    "\n",
    "if name == \"DBLP\":\n",
    "    dataset = HGBDataset(root+name, name)\n",
    "    data = dataset.data\n",
    "    print(data)\n",
    "    data[\"paper\"][\"y2\"] = torch.rand(data[\"paper\"].num_nodes) > 0.5\n",
    "    data['venue'].x = torch.arange(data['venue'].num_nodes).reshape(-1, 1)\n",
    "    rns = RandomNodeSplit(num_val=0.1, num_test=0.1, key = \"y2\")\n",
    "    targets = {\"paper\"}\n",
    "    #leave_out = {\"venue\"}\n",
    "    \n",
    "elif name == \"ACM\":\n",
    "    dataset = HGBDataset(root+name, name)\n",
    "    data = dataset.data\n",
    "    print(data)\n",
    "    data[\"paper\"][\"y\"] = torch.rand(data[\"paper\"].num_nodes) > 0.5\n",
    "    data['term'].x = torch.arange(data['term'].num_nodes).reshape(-1, 1)\n",
    "    rns = RandomNodeSplit(num_val=0.1, num_test=0.1, key = \"y\")\n",
    "    targets = {\"paper\"}\n",
    "\n",
    "elif name == \"IMDB\":\n",
    "    #dataset = IMDB(root+name)\n",
    "    dataset = HGBDataset(root+name, name)\n",
    "    data = dataset.data\n",
    "    rns = RandomNodeSplit(num_val=0.1, num_test=0.1)\n",
    "    data['keyword'].x = torch.arange(data['keyword'].num_nodes).reshape(-1, 1)\n",
    "    targets = {\"movie\"}   \n",
    "\n",
    "print(data)\n",
    "print(data.num_nodes)\n",
    "print(data.num_edges)\n",
    "splits = rns(data.clone())\n",
    "\n",
    "\n",
    "train_nodes = {}\n",
    "valid_nodes = {}\n",
    "for nt in data.node_types:\n",
    "    if nt in targets:\n",
    "        train_nodes[nt] = set(splits[nt].train_mask.nonzero().flatten().numpy())\n",
    "        valid_nodes[nt] = set(splits[nt].val_mask.nonzero().flatten().numpy())\n",
    "\n",
    "\n",
    "data = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485fc2ac-2fb0-4a6e-9ead-cb841a467ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transductive_edges = {et: [] for et in data.edge_types}\n",
    "inductive_edges = {et: [] for et in data.edge_types}\n",
    "transductive_nodes = {nt: set() for nt in data.node_types}\n",
    "inductive_nodes = {nt: set() for nt in data.node_types}\n",
    "for et in data.edge_types:\n",
    "    st, _, tt = et\n",
    "    for u, v in data[et].edge_index.numpy().T:\n",
    "        if st in valid_nodes and u in valid_nodes[st]:\n",
    "            if tt not in targets :\n",
    "                inductive_edges[et].append((u, v))\n",
    "                inductive_nodes[st].add(u)\n",
    "        if tt in valid_nodes and v in valid_nodes[tt]:\n",
    "            if st not in targets:\n",
    "                inductive_edges[et].append((u, v))\n",
    "                inductive_nodes[tt].add(v)\n",
    "        else:\n",
    "            transductive_edges[et].append((u, v))\n",
    "            transductive_nodes[st].add(u)\n",
    "            transductive_nodes[tt].add(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862694f0-5495-4d27-b02a-786fa28368dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "trans_hdata = HeteroData()\n",
    "ind_hdata = HeteroData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356c38c-82cf-4c85-9c63-228347e0d933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c96b94-2ad8-4b7a-b68c-423c03e51cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124022/3145845518.py:11: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2981.)\n",
      "  inductive_edges[k] = torch.tensor(list(inductive_edges[k])).long().T\n"
     ]
    }
   ],
   "source": [
    "for k in transductive_nodes:\n",
    "    transductive_nodes[k] = torch.tensor(list(transductive_nodes[k])).long()\n",
    "    inductive_nodes[k] = torch.tensor(list(inductive_nodes[k])).long()\n",
    "    trans_hdata[k].x = data[k].x\n",
    "    ind_hdata[k].x = data[k].x\n",
    "    trans_hdata[k].transductive_nodes = transductive_nodes[k]\n",
    "    ind_hdata[k].inductive_nodes = inductive_nodes[k]\n",
    "\n",
    "for k in transductive_edges:\n",
    "    transductive_edges[k] = torch.tensor(list(transductive_edges[k])).long().T\n",
    "    inductive_edges[k] = torch.tensor(list(inductive_edges[k])).long().T\n",
    "    trans_hdata[k].edge_index = transductive_edges[k]\n",
    "    if len(inductive_edges[k])!=0:\n",
    "        ind_hdata[k].edge_index = inductive_edges[k] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d0b18d7-1636-4c5d-8400-33657843a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection_ = {}\n",
    "# for nt in data.node_types:\n",
    "#     intersection_[nt] = transductive_nodes[nt].intersection(inductive_nodes[nt])\n",
    "#     inductive_nodes[nt] = inductive_nodes[nt].difference(transductive_nodes[nt])\n",
    "#    transductive_nodes[nt] = transductive_nodes[nt].union(intersection_[nt]) \n",
    "\n",
    "\n",
    "\n",
    "#train_data = data.clone().subgraph(transductive_nodes)\n",
    "#valid_data = data.clone().subgraph(inductive_nodes)\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "116eb47e-3c5e-4174-b1dc-0e870cdd5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "if os.path.isfile('datasplits/'+'ind'+name+'_train_data.pickle'):\n",
    "        train_data = open_file('datasplits/'+'ind'+name+'_train_data.pickle')\n",
    "        trans_train_data = open_file('datasplits/'+'ind'+name+'_train_train_data.pickle')\n",
    "        trans_valid_data = open_file('datasplits/'+'ind'+name+'_train_valid_data.pickle')\n",
    "        valid_data = open_file('datasplits/'+'ind'+name+'_valid_data.pickle')\n",
    "        test_data = open_file('datasplits/'+'ind'+name+'_test_data.pickle')\n",
    "        \n",
    "else:    \n",
    "    \n",
    "    rlp = RandomLinkSplit(edge_types=trans_hdata.edge_types,\n",
    "    num_val=0.1, num_test=0)\n",
    "\n",
    "    trans_train_data, trans_valid_data , _ = rlp(trans_hdata)\n",
    "    rlp2 = RandomLinkSplit(\n",
    "         edge_types=trans_hdata.edge_types,\n",
    "         num_val=0.0, num_test=0.0)\n",
    "\n",
    "    train_data, _ , _ = rlp2(trans_hdata)\n",
    "\n",
    "\n",
    "    rlp2 = RandomLinkSplit(\n",
    "         edge_types=ind_hdata.edge_types,\n",
    "         num_val=0.5, num_test=0.0)\n",
    "\n",
    "    valid_data, test_data , _ = rlp2(ind_hdata)\n",
    "\n",
    "\n",
    "    save_file(train_data, '/home/aemad/PycharmProjects/project_slkd/datasplits/'+'ind'+name+'_train_data.pickle')\n",
    "    save_file(trans_train_data, '/home/aemad/PycharmProjects/project_slkd/datasplits/'+'ind'+name+'_train_train_data.pickle')\n",
    "    save_file(trans_valid_data,'/home/aemad/PycharmProjects/project_slkd/datasplits/'+ 'ind'+name+'_train_valid_data.pickle')\n",
    "    save_file(valid_data, '/home/aemad/PycharmProjects/project_slkd/datasplits/'+ 'ind'+name+'_valid_data.pickle')\n",
    "    save_file(test_data, '/home/aemad/PycharmProjects/project_slkd/datasplits/'+'ind'+name+'_test_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d4eef8-3aaa-4e13-ba97-e01d5abeca31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mpaper\u001b[0m={\n",
       "    x=[3025, 1902],\n",
       "    inductive_nodes=[302]\n",
       "  },\n",
       "  \u001b[1mauthor\u001b[0m={\n",
       "    x=[5959, 1902],\n",
       "    inductive_nodes=[0]\n",
       "  },\n",
       "  \u001b[1msubject\u001b[0m={\n",
       "    x=[56, 1902],\n",
       "    inductive_nodes=[0]\n",
       "  },\n",
       "  \u001b[1mterm\u001b[0m={\n",
       "    x=[1902, 1],\n",
       "    inductive_nodes=[0]\n",
       "  },\n",
       "  \u001b[1m(paper, to, author)\u001b[0m={\n",
       "    edge_index=[2, 472],\n",
       "    edge_label=[944],\n",
       "    edge_label_index=[2, 944]\n",
       "  },\n",
       "  \u001b[1m(author, to, paper)\u001b[0m={\n",
       "    edge_index=[2, 472],\n",
       "    edge_label=[944],\n",
       "    edge_label_index=[2, 944]\n",
       "  },\n",
       "  \u001b[1m(paper, to, subject)\u001b[0m={\n",
       "    edge_index=[2, 151],\n",
       "    edge_label=[302],\n",
       "    edge_label_index=[2, 302]\n",
       "  },\n",
       "  \u001b[1m(subject, to, paper)\u001b[0m={\n",
       "    edge_index=[2, 151],\n",
       "    edge_label=[302],\n",
       "    edge_label_index=[2, 302]\n",
       "  },\n",
       "  \u001b[1m(paper, to, term)\u001b[0m={\n",
       "    edge_index=[2, 12721],\n",
       "    edge_label=[25442],\n",
       "    edge_label_index=[2, 25442]\n",
       "  },\n",
       "  \u001b[1m(term, to, paper)\u001b[0m={\n",
       "    edge_index=[2, 12721],\n",
       "    edge_label=[25442],\n",
       "    edge_label_index=[2, 25442]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b253b92-b587-4701-ad12-a575f26d22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transductive_dict = None\n",
    "if name == \"DBLP\":\n",
    "    transductive_dict = {'venue': data['venue'].num_nodes}\n",
    "elif name == \"IMDB\":\n",
    "    transductive_dict = {'keyword': data['keyword'].num_nodes}\n",
    "elif name == \"ACM\":\n",
    "    transductive_dict = {'term': data['term'].num_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffd28aa6-12d9-4092-b150-6f310c8b4594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1500, Training loss: 93.3683, AUC: 0.5429131794780448, AP: 0.5785098380948351\n",
      "Epoch 20/1500, Training loss: 61.5078, AUC: 0.6861153240665745, AP: 0.6875650928180461\n",
      "Epoch 30/1500, Training loss: 48.4648, AUC: 0.6883168568937893, AP: 0.6967808752395668\n",
      "Epoch 40/1500, Training loss: 38.3688, AUC: 0.6129646280155809, AP: 0.6533083802063642\n",
      "Epoch 50/1500, Training loss: 27.0629, AUC: 0.7599678509546677, AP: 0.7707666934032942\n",
      "Epoch 60/1500, Training loss: 14.5793, AUC: 0.7790343026170077, AP: 0.7832896469026022\n",
      "Epoch 70/1500, Training loss: 10.0506, AUC: 0.8508496802077189, AP: 0.8439043060444225\n",
      "Epoch 80/1500, Training loss: 11.5238, AUC: 0.7973443037771485, AP: 0.7961008289807163\n",
      "Epoch 90/1500, Training loss: 40.5654, AUC: 0.776046557208892, AP: 0.7725669616250112\n",
      "Epoch 100/1500, Training loss: 29.5420, AUC: 0.7903936263028998, AP: 0.781425528327655\n",
      "Epoch 110/1500, Training loss: 24.0872, AUC: 0.8336384294981589, AP: 0.8264804425066182\n",
      "Epoch 120/1500, Training loss: 11.0424, AUC: 0.8150166060409835, AP: 0.8096562468858779\n",
      "Epoch 130/1500, Training loss: 10.2482, AUC: 0.9048119849641912, AP: 0.9000612451721715\n",
      "Epoch 140/1500, Training loss: 8.6810, AUC: 0.8915644832837168, AP: 0.8853265633990963\n",
      "Epoch 150/1500, Training loss: 7.0335, AUC: 0.9043215305803597, AP: 0.8993171026524908\n",
      "Epoch 160/1500, Training loss: 5.5536, AUC: 0.9127753705443229, AP: 0.9067369468367256\n",
      "Epoch 170/1500, Training loss: 4.5797, AUC: 0.9237331201773032, AP: 0.9200353020029881\n",
      "Epoch 180/1500, Training loss: 4.1469, AUC: 0.9271106111431415, AP: 0.9270086344632105\n",
      "Epoch 190/1500, Training loss: 3.9264, AUC: 0.9306635990355696, AP: 0.9310702520854333\n",
      "Epoch 200/1500, Training loss: 3.7297, AUC: 0.9337409550481134, AP: 0.9340949077798734\n",
      "Epoch 210/1500, Training loss: 3.5901, AUC: 0.9356409772762174, AP: 0.9357064783879546\n",
      "Epoch 220/1500, Training loss: 3.4643, AUC: 0.9373195282974721, AP: 0.9373336973602203\n",
      "Epoch 230/1500, Training loss: 3.3521, AUC: 0.9387493964547444, AP: 0.9386374343539513\n",
      "Epoch 240/1500, Training loss: 3.2482, AUC: 0.9400500463598875, AP: 0.939782401676478\n",
      "Epoch 250/1500, Training loss: 3.1509, AUC: 0.9411647783320872, AP: 0.9406752182385567\n",
      "Epoch 260/1500, Training loss: 3.0594, AUC: 0.9421581361725753, AP: 0.9414706670342609\n",
      "Epoch 270/1500, Training loss: 2.9730, AUC: 0.9430861415685976, AP: 0.9423002874210682\n",
      "Epoch 280/1500, Training loss: 2.8913, AUC: 0.9438482612023409, AP: 0.9428303564700296\n",
      "Epoch 290/1500, Training loss: 2.8139, AUC: 0.9446177112681579, AP: 0.9434667715176842\n",
      "Epoch 300/1500, Training loss: 2.7404, AUC: 0.9452701880427599, AP: 0.9440006558709441\n",
      "Epoch 310/1500, Training loss: 2.6706, AUC: 0.9459148416130271, AP: 0.9445462986458256\n",
      "Epoch 320/1500, Training loss: 2.6042, AUC: 0.9465298733295434, AP: 0.9450744676801425\n",
      "Epoch 330/1500, Training loss: 2.5409, AUC: 0.9471414049902538, AP: 0.9455881513363021\n",
      "Epoch 340/1500, Training loss: 2.4806, AUC: 0.9476911248459521, AP: 0.9459710861118587\n",
      "Epoch 350/1500, Training loss: 2.4231, AUC: 0.9482353113091174, AP: 0.9464554060009969\n",
      "Epoch 360/1500, Training loss: 2.3681, AUC: 0.9487424912296524, AP: 0.94684328024379\n",
      "Epoch 370/1500, Training loss: 2.3156, AUC: 0.9492576430027108, AP: 0.9472688093228306\n",
      "Epoch 380/1500, Training loss: 2.2653, AUC: 0.9497589937898904, AP: 0.94764861078522\n",
      "Epoch 390/1500, Training loss: 2.2173, AUC: 0.9502542824009672, AP: 0.9480508287923555\n",
      "Epoch 400/1500, Training loss: 2.1713, AUC: 0.9507367789620536, AP: 0.9484467919598117\n",
      "Epoch 410/1500, Training loss: 2.1273, AUC: 0.9511776217172403, AP: 0.9488194994083927\n",
      "Epoch 420/1500, Training loss: 2.0852, AUC: 0.9516125979251032, AP: 0.9491919324878736\n",
      "Epoch 430/1500, Training loss: 2.0448, AUC: 0.9520586915779936, AP: 0.949568442169969\n",
      "Epoch 440/1500, Training loss: 2.0061, AUC: 0.9524753508609718, AP: 0.9499164155919568\n",
      "Epoch 450/1500, Training loss: 1.9689, AUC: 0.9529040818740705, AP: 0.9502813155648389\n",
      "Epoch 460/1500, Training loss: 1.9331, AUC: 0.9533178311633266, AP: 0.950628316815014\n",
      "Epoch 470/1500, Training loss: 1.8985, AUC: 0.9536965934116706, AP: 0.9509363966747953\n",
      "Epoch 480/1500, Training loss: 1.8652, AUC: 0.9540872965791956, AP: 0.9512601184416243\n",
      "Epoch 490/1500, Training loss: 1.8330, AUC: 0.954470923481289, AP: 0.9515519434273791\n",
      "Epoch 500/1500, Training loss: 1.8018, AUC: 0.9548538485926035, AP: 0.9518435111533685\n",
      "Epoch 510/1500, Training loss: 1.7716, AUC: 0.9552421545726134, AP: 0.9522078162080428\n",
      "Epoch 520/1500, Training loss: 1.7423, AUC: 0.9555986625806965, AP: 0.9524886823065172\n",
      "Epoch 530/1500, Training loss: 1.7139, AUC: 0.9559604840244618, AP: 0.9527859785550414\n",
      "Epoch 540/1500, Training loss: 1.6863, AUC: 0.9563130576786492, AP: 0.953071036503614\n",
      "Epoch 550/1500, Training loss: 1.6594, AUC: 0.9566435137757523, AP: 0.953323124306489\n",
      "Epoch 560/1500, Training loss: 1.6332, AUC: 0.9569673186651448, AP: 0.9535675486718205\n",
      "Epoch 570/1500, Training loss: 1.6076, AUC: 0.9572995833702846, AP: 0.9538377311122501\n",
      "Epoch 580/1500, Training loss: 1.5826, AUC: 0.9576447000271613, AP: 0.9541931827417393\n",
      "Epoch 590/1500, Training loss: 1.5581, AUC: 0.9579605186574575, AP: 0.9544272739419055\n",
      "Epoch 600/1500, Training loss: 1.5342, AUC: 0.9582869969312565, AP: 0.9546995773054064\n",
      "Epoch 610/1500, Training loss: 1.5106, AUC: 0.9585863092515051, AP: 0.9549278071378366\n",
      "Epoch 620/1500, Training loss: 1.4876, AUC: 0.958897569067782, AP: 0.9551727009389559\n",
      "Epoch 630/1500, Training loss: 1.4649, AUC: 0.9591858000978388, AP: 0.955432235525753\n",
      "Epoch 640/1500, Training loss: 1.4426, AUC: 0.9594810943590495, AP: 0.9556706250578367\n",
      "Epoch 650/1500, Training loss: 1.4208, AUC: 0.9597596977375067, AP: 0.9559092588377691\n",
      "Epoch 660/1500, Training loss: 1.3993, AUC: 0.9600573575543965, AP: 0.9561344298371688\n",
      "Epoch 670/1500, Training loss: 1.3781, AUC: 0.9603349658693907, AP: 0.9563277006578945\n",
      "Epoch 680/1500, Training loss: 1.3574, AUC: 0.9606007710867452, AP: 0.9565308274174258\n",
      "Epoch 690/1500, Training loss: 1.3369, AUC: 0.9608491977078111, AP: 0.9567042805520127\n",
      "Epoch 700/1500, Training loss: 1.3168, AUC: 0.961125781308147, AP: 0.9569333470769648\n",
      "Epoch 710/1500, Training loss: 1.2970, AUC: 0.9613690859601302, AP: 0.9571242902081286\n",
      "Epoch 720/1500, Training loss: 1.2775, AUC: 0.9616131707403358, AP: 0.9572961567897931\n",
      "Epoch 730/1500, Training loss: 1.2584, AUC: 0.9618430379248325, AP: 0.9574528032692955\n",
      "Epoch 740/1500, Training loss: 1.2395, AUC: 0.9620818328444296, AP: 0.9576361821628494\n",
      "Epoch 750/1500, Training loss: 1.2209, AUC: 0.9623138968654743, AP: 0.9578128215364489\n",
      "Epoch 760/1500, Training loss: 1.2026, AUC: 0.9625360349867776, AP: 0.957969911490475\n",
      "Epoch 770/1500, Training loss: 1.1846, AUC: 0.9627536638943527, AP: 0.9581237862211938\n",
      "Epoch 780/1500, Training loss: 1.1669, AUC: 0.9629606962805981, AP: 0.9582620719719754\n",
      "Epoch 790/1500, Training loss: 1.1494, AUC: 0.9631643230955731, AP: 0.9584254699486493\n",
      "Epoch 800/1500, Training loss: 1.1322, AUC: 0.9633540964099876, AP: 0.9585116407371521\n",
      "Epoch 810/1500, Training loss: 1.1152, AUC: 0.9635304650793264, AP: 0.9585976557294995\n",
      "Epoch 820/1500, Training loss: 1.0985, AUC: 0.9637034046958389, AP: 0.9587234036801566\n",
      "Epoch 830/1500, Training loss: 1.0821, AUC: 0.9638760476949932, AP: 0.9588267340129784\n",
      "Epoch 840/1500, Training loss: 1.0659, AUC: 0.9640286559366161, AP: 0.958964032551915\n",
      "Epoch 850/1500, Training loss: 1.0499, AUC: 0.9641707029005845, AP: 0.9590447352752365\n",
      "Epoch 860/1500, Training loss: 1.0342, AUC: 0.9643260486524645, AP: 0.9591222315902741\n",
      "Epoch 870/1500, Training loss: 1.0187, AUC: 0.9644571325715389, AP: 0.9591935307032831\n",
      "Epoch 880/1500, Training loss: 1.0034, AUC: 0.9646093285970836, AP: 0.9592716596293761\n",
      "Epoch 890/1500, Training loss: 0.9883, AUC: 0.964749775456222, AP: 0.9593320544821551\n",
      "Epoch 900/1500, Training loss: 0.9734, AUC: 0.9648688560565368, AP: 0.9593448420249989\n",
      "Epoch 910/1500, Training loss: 0.9587, AUC: 0.965002430907572, AP: 0.9593914896479606\n",
      "Epoch 920/1500, Training loss: 0.9442, AUC: 0.9651199867964363, AP: 0.959448258117932\n",
      "Epoch 930/1500, Training loss: 0.9299, AUC: 0.9652356839342058, AP: 0.9594909114774579\n",
      "Epoch 940/1500, Training loss: 0.9158, AUC: 0.9653550257829925, AP: 0.9595340979731366\n",
      "Epoch 950/1500, Training loss: 0.9019, AUC: 0.9654638831031128, AP: 0.9595566034410664\n",
      "Epoch 960/1500, Training loss: 0.8881, AUC: 0.9655816079599313, AP: 0.9596010302231814\n",
      "Epoch 970/1500, Training loss: 0.8745, AUC: 0.9656995849541403, AP: 0.9596498377122247\n",
      "Epoch 980/1500, Training loss: 0.8611, AUC: 0.9657980763668946, AP: 0.9596602453359369\n",
      "Epoch 990/1500, Training loss: 0.8478, AUC: 0.9658984137149108, AP: 0.9597056127889854\n",
      "Epoch 1000/1500, Training loss: 0.8347, AUC: 0.9659932460421945, AP: 0.9597092532774698\n",
      "Epoch 1010/1500, Training loss: 0.8218, AUC: 0.9660851493377138, AP: 0.9597464543833445\n",
      "Epoch 1020/1500, Training loss: 0.8090, AUC: 0.9661758227806864, AP: 0.9597360415107942\n",
      "Epoch 1030/1500, Training loss: 0.7964, AUC: 0.9662691046445694, AP: 0.9597539953096064\n",
      "Epoch 1040/1500, Training loss: 0.7839, AUC: 0.9663565276500867, AP: 0.9597490397626114\n",
      "Epoch 1050/1500, Training loss: 0.7716, AUC: 0.9664514988421854, AP: 0.9597915206400668\n",
      "Epoch 1060/1500, Training loss: 0.7594, AUC: 0.9665295640577962, AP: 0.9598035296590357\n",
      "Epoch 1070/1500, Training loss: 0.7474, AUC: 0.966602887435773, AP: 0.9598029234096696\n",
      "Epoch 1080/1500, Training loss: 0.7355, AUC: 0.966693057525262, AP: 0.9598182287981399\n",
      "Epoch 1090/1500, Training loss: 0.7238, AUC: 0.9667743737150905, AP: 0.9598302198280536\n",
      "Epoch 1100/1500, Training loss: 0.7122, AUC: 0.9668608559108576, AP: 0.9598350558524429\n",
      "Epoch 1110/1500, Training loss: 0.7008, AUC: 0.9669265394590316, AP: 0.9598205586567441\n",
      "Epoch 1120/1500, Training loss: 0.6895, AUC: 0.9669852417055489, AP: 0.9598077175181388\n",
      "Epoch 1130/1500, Training loss: 0.6783, AUC: 0.967060924193891, AP: 0.9598198855196752\n",
      "Epoch 1140/1500, Training loss: 0.6674, AUC: 0.9671325780309799, AP: 0.9598106527798281\n",
      "Epoch 1150/1500, Training loss: 0.6565, AUC: 0.9671880629486045, AP: 0.9598049596534184\n",
      "Epoch 1160/1500, Training loss: 0.6458, AUC: 0.9672430495103257, AP: 0.9597832528951449\n",
      "Epoch 1170/1500, Training loss: 0.6352, AUC: 0.967303737365738, AP: 0.9597649354510165\n",
      "Epoch 1180/1500, Training loss: 0.6248, AUC: 0.9673483000138078, AP: 0.9597202265572092\n",
      "Epoch 1190/1500, Training loss: 0.6145, AUC: 0.9673991311604362, AP: 0.9596789673551596\n",
      "Epoch 1200/1500, Training loss: 0.6044, AUC: 0.9674363950373445, AP: 0.9596260148239673\n",
      "Epoch 1210/1500, Training loss: 0.5944, AUC: 0.9674994333956841, AP: 0.959625868068353\n",
      "Epoch 1220/1500, Training loss: 0.5845, AUC: 0.9675522599851837, AP: 0.9596089061042931\n",
      "Epoch 1230/1500, Training loss: 0.5748, AUC: 0.9675928361288398, AP: 0.9595802597949002\n",
      "Epoch 1240/1500, Training loss: 0.5652, AUC: 0.967627051807576, AP: 0.9595419007226846\n",
      "Epoch 1250/1500, Training loss: 0.5558, AUC: 0.9676699568418968, AP: 0.9595303373073389\n",
      "Epoch 1260/1500, Training loss: 0.5465, AUC: 0.9677097664405332, AP: 0.9595026516379609\n",
      "Epoch 1270/1500, Training loss: 0.5373, AUC: 0.967750236640483, AP: 0.9594650289594167\n",
      "Epoch 1280/1500, Training loss: 0.5283, AUC: 0.9677856592948839, AP: 0.9594296412998332\n",
      "Epoch 1290/1500, Training loss: 0.5194, AUC: 0.9678112600337495, AP: 0.9593772168946945\n",
      "Epoch 1300/1500, Training loss: 0.5106, AUC: 0.9678339387085997, AP: 0.9593412614964696\n",
      "Epoch 1310/1500, Training loss: 0.5020, AUC: 0.9678408831961274, AP: 0.9592582964454414\n",
      "Epoch 1320/1500, Training loss: 0.4935, AUC: 0.9678641753779585, AP: 0.9592173232376253\n",
      "Epoch 1330/1500, Training loss: 0.4852, AUC: 0.967879257396541, AP: 0.9591560463071253\n",
      "Epoch 1340/1500, Training loss: 0.4769, AUC: 0.967899781755083, AP: 0.9591070708852705\n",
      "Epoch 1350/1500, Training loss: 0.4688, AUC: 0.9679116318418424, AP: 0.9590680210651751\n",
      "Epoch 1360/1500, Training loss: 0.4609, AUC: 0.967921125098408, AP: 0.9590165204383734\n",
      "Epoch 1370/1500, Training loss: 0.4530, AUC: 0.9679300666847048, AP: 0.958986358028752\n",
      "Epoch 1380/1500, Training loss: 0.4453, AUC: 0.967950270909845, AP: 0.9589584241828374\n",
      "Epoch 1390/1500, Training loss: 0.4377, AUC: 0.9679558707595977, AP: 0.9588980100559512\n",
      "Epoch 1400/1500, Training loss: 0.4302, AUC: 0.9679632396630145, AP: 0.9588539496406934\n",
      "Epoch 1410/1500, Training loss: 0.4229, AUC: 0.9679518512174808, AP: 0.9587968499153564\n",
      "Epoch 1420/1500, Training loss: 0.4157, AUC: 0.9679424316084617, AP: 0.958729865692016\n",
      "Epoch 1430/1500, Training loss: 0.4085, AUC: 0.9679462073915803, AP: 0.9586949951208245\n",
      "Epoch 1440/1500, Training loss: 0.4015, AUC: 0.9679393599666031, AP: 0.9586311249866027\n",
      "Epoch 1450/1500, Training loss: 0.3947, AUC: 0.9679362864434349, AP: 0.9585671461767373\n",
      "Epoch 1460/1500, Training loss: 0.3879, AUC: 0.967917644631166, AP: 0.9584801256332689\n",
      "Epoch 1470/1500, Training loss: 0.3813, AUC: 0.9679111830640107, AP: 0.9584297160819809\n",
      "Epoch 1480/1500, Training loss: 0.3747, AUC: 0.9678899152009538, AP: 0.9583327793720446\n",
      "Epoch 1490/1500, Training loss: 0.3683, AUC: 0.9678860549110685, AP: 0.9582833729558378\n",
      "Epoch 1500/1500, Training loss: 0.3620, AUC: 0.9678710355120229, AP: 0.9582171268621336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "teacher = Teacher(trans_train_data.metadata(), output_dim=256, transductive_types= transductive_dict, device = device).to(device)\n",
    "epochs = 1500\n",
    "best = 0.0\n",
    "optimizer = torch.optim.Adam(teacher.parameters(), lr=0.001)\n",
    "for epoch in range(epochs):\n",
    "     teacher.train()\n",
    "     z = teacher(trans_train_data)\n",
    "     loss = teacher.loss(trans_train_data, z, batch_size=0)  \n",
    "        \n",
    "     optimizer.zero_grad()\n",
    "     loss.backward()\n",
    "     optimizer.step()\n",
    "     \n",
    "     #val_loss = teacher.train_step(x_=data.x, edge_index=train_data.edge_index, negatives=negatives, grad=False)\n",
    "     teacher.eval()\n",
    "     z = teacher(trans_train_data)\n",
    "     auc, ap = evaluate(z, trans_valid_data, trans_valid_data.metadata()[1])\n",
    "     \n",
    "     if (epoch + 1) % 10 == 0:\n",
    "         print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "               f\"Training loss: {loss:.4f}, \"\n",
    "               f\"AUC: {auc}, AP: {ap}\")\n",
    "     if ap > best:\n",
    "        best = ap\n",
    "        teacher.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27db19b3-2e5b-46da-a3bd-1ff18254f609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation AUC: 0.966885558180649, AP: 0.9598415721268482\n"
     ]
    }
   ],
   "source": [
    "teacher = Teacher(trans_train_data.metadata(), output_dim=256 , transductive_types=transductive_dict, device = device).to(device)\n",
    "teacher.load()\n",
    "teacher.eval()\n",
    "z = teacher(trans_train_data)\n",
    "auc, ap = evaluate(z, trans_valid_data, trans_valid_data.metadata()[1])\n",
    "print(f\"validation AUC: {auc}, AP: {ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dd93d20-0e8f-42bf-b877-a0fff673b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nt in transductive_dict:\n",
    "#     trans_train_data[nt].x = teacher.encoder.em_dict[nt].weight.data.detach().cpu()[trans_train_data[nt].x.squeeze()]\n",
    "#     trans_valid_data[nt].x = teacher.encoder.em_dict[nt].weight.data.detach().cpu()[trans_valid_data[nt].x.squeeze()]\n",
    "#     valid_data[nt].x = teacher.encoder.em_dict[nt].weight.data.detach().cpu()[valid_data[nt].x.squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5aad061-79a4-4e04-a890-c2e886cb39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, node_types, transductive_types = None, device = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.em_dict = nn.ModuleDict()\n",
    "        self.lin1 = nn.ModuleDict()\n",
    "        self.bn1 = nn.ModuleDict()\n",
    "        self.lin2 = nn.ModuleDict()\n",
    "        self.bn2 = nn.ModuleDict()\n",
    "        self.tt = transductive_types\n",
    "        \n",
    "        for nt in node_types:\n",
    "            if transductive_types is not None and nt in transductive_types:\n",
    "                self.em_dict[nt] = nn.Embedding(transductive_types[nt], 128) \n",
    "                nn.init.xavier_uniform_(self.em_dict[nt].weight)\n",
    "            else:\n",
    "                self.em_dict[nt] = None\n",
    "                \n",
    "            self.lin1[nt] = Linear(input_dim, output_dim)\n",
    "            self.bn1[nt] = BatchNorm(output_dim)\n",
    "            self.lin2[nt] = Linear(output_dim, output_dim)\n",
    "            self.bn2[nt] = BatchNorm(output_dim)\n",
    "                \n",
    "    def forward(self, x_dict):\n",
    "        for node_type in x_dict:\n",
    "            if self.em_dict[node_type] is not None:\n",
    "                x_dict[node_type] = self.em_dict[node_type](x_dict[node_type].to(self.device).squeeze()) \n",
    "               \n",
    "            x_dict[node_type] = self.lin1[node_type](x_dict[node_type].to(self.device))\n",
    "            x_dict[node_type] = self.bn1[node_type](x_dict[node_type])\n",
    "            x_dict[node_type] = self.lin2[node_type](x_dict[node_type])\n",
    "            x_dict[node_type] = self.bn2[node_type](x_dict[node_type])\n",
    "            \n",
    "        return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "321f8bed-c824-41ee-8ca0-f0cd4c183706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "testing loss: 4.39057207, testing AUC: 0.9623979523561278, AP: 0.9440975883123834\n"
     ]
    }
   ],
   "source": [
    "distill = True\n",
    "#student = Student(-1, 128, trans_train_data.node_types, transductive_dict, device).to(device)\n",
    "\n",
    "# if distill:\n",
    "#      for nt in transductive_dict:\n",
    "#          student.em_dict[nt].weight.data = teacher.encoder.em_dict[nt].weight.detach()\n",
    "        \n",
    "torch.manual_seed(10)\n",
    "#optim = torch.optim.Adam(student.parameters(), lr=0.01)\n",
    "\n",
    "T = 10\n",
    "criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "losses = []\n",
    "epochs = 1000\n",
    "best = 0.0\n",
    "best_student = None\n",
    "beta = [1]\n",
    "gamma = [1]\n",
    "for b in beta:\n",
    "    for q in gamma:\n",
    "        best = 0.0\n",
    "        best_student = None\n",
    "        losses = []\n",
    "        student = Student(-1, 256, trans_train_data.node_types, transductive_dict, device).to(device)\n",
    "        optim = torch.optim.Adam(student.parameters(), lr=0.01)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            student_loss = 0.0\n",
    "            student.train()\n",
    "            torch.manual_seed(10)\n",
    "            student_x = student(trans_train_data.x_dict)\n",
    "            if distill:\n",
    "                for nt in trans_train_data.node_types:\n",
    "                      student_loss +=  q * F.mse_loss(student_x[nt], z[nt].detach())\n",
    "            student_loss +=  b * e_loss(trans_train_data, student_x, trans_train_data.edge_types, device)\n",
    "            optim.zero_grad()\n",
    "            student_loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(student_loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                student.eval()\n",
    "                student_x = student(valid_data.x_dict)    \n",
    "                val_loss = e_loss(valid_data, student_x, valid_data.edge_types, device)\n",
    "                auc, ap = evaluate(student_x, valid_data, valid_data.edge_types)\n",
    "\n",
    "            if ap > best:\n",
    "                best = ap\n",
    "                best_student = student\n",
    "                save_file(student, 'ind'+name+'_student.pickle')\n",
    "\n",
    "            # if (epoch + 1) % 10 == 0:\n",
    "            #     print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "            #       f\"student loss: {student_loss.item():.8f}, \"\n",
    "            #       f\"val loss: {val_loss.item():.8f}, \"    \n",
    "            #       f\"validation AUC: {auc}, AP: {ap}\")\n",
    "                \n",
    "        # testing       \n",
    "        with torch.no_grad():\n",
    "            best_student.eval()\n",
    "            student_x = best_student(test_data.x_dict)    \n",
    "            auc, ap = evaluate(student_x, test_data, test_data.edge_types)\n",
    "            print(b, q)\n",
    "            print(f\"testing loss: {val_loss.item():.8f}, \"    \n",
    "              f\"testing AUC: {auc}, AP: {ap}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86e187e0-453f-4658-81a8-a2a8e2447318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss: 4.39057207, testing AUC: 0.9623979523561278, AP: 0.9440975883123834\n"
     ]
    }
   ],
   "source": [
    "# testing       \n",
    "with torch.no_grad():\n",
    "    best_student.eval()\n",
    "    student_x = best_student(test_data.x_dict)    \n",
    "    auc, ap = evaluate(student_x, test_data, test_data.edge_types)\n",
    "\n",
    "    print(f\"testing loss: {val_loss.item():.8f}, \"    \n",
    "      f\"testing AUC: {auc}, AP: {ap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dddc76-2dc5-44d2-89d0-7ac34e61abb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869aae03-e707-4801-bf0a-542b122c59d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fdf13c-b9df-4ad2-8193-4ca2a18edb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec39ec-90a0-4c35-b771-4b3f2b0aeca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cac909-1453-4b0b-bcb6-1dd5001f312c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf704640-29b9-41be-b9e6-38e87da38eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
